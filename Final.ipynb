{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aed36392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import activations, optimizers, losses\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a392d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_Name</th>\n",
       "      <th>Text Features Type</th>\n",
       "      <th>Train Acccurcay</th>\n",
       "      <th>CV_accuracy</th>\n",
       "      <th>Test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Bag of word(Unigram)+sentiment_score</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBM Classifier</td>\n",
       "      <td>Bag of word(Unigram)+sentiment_score</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest Classifier</td>\n",
       "      <td>Bag of word(Unigram)+sentiment_score</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost Classifier</td>\n",
       "      <td>Bag of word(Unigram)+sentiment_score</td>\n",
       "      <td>0.659</td>\n",
       "      <td>-</td>\n",
       "      <td>0.657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Bag of word(Bigram)+sentiment_score</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LightGBM Classifier</td>\n",
       "      <td>Bag of word(Bigram)+sentiment_score</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RandomForest Classifier</td>\n",
       "      <td>Bag of word(Bigram)+sentiment_score</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBoost Classifier</td>\n",
       "      <td>Bag of word(Bigram)+sentiment_score</td>\n",
       "      <td>0.602</td>\n",
       "      <td>-</td>\n",
       "      <td>0.599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TF-IDF(Uni+Bigram)+sentiment_score</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LightGBM Classifier</td>\n",
       "      <td>TF-IDF(Uni+Bigram)+sentiment_score</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RandomForest Classifier</td>\n",
       "      <td>TF-IDF(Uni+Bigram)+sentiment_score</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LSTM+Dense</td>\n",
       "      <td>Text_sequnece_padded</td>\n",
       "      <td>0.705</td>\n",
       "      <td>-</td>\n",
       "      <td>0.710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bert_pretained_model</td>\n",
       "      <td>Bert Tokeinizer</td>\n",
       "      <td>0.652</td>\n",
       "      <td>-</td>\n",
       "      <td>0.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TFDistilBertForSequenceClassification</td>\n",
       "      <td>DistilBertTokenizer(for cleaned text)</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TFDistilBertForSequenceClassification</td>\n",
       "      <td>DistilBertTokenizer(without cleaned text)</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Model_Name  \\\n",
       "1                     Logistic Regression   \n",
       "2                     LightGBM Classifier   \n",
       "3                 RandomForest Classifier   \n",
       "4                      XGBoost Classifier   \n",
       "5                     Logistic Regression   \n",
       "6                     LightGBM Classifier   \n",
       "7                 RandomForest Classifier   \n",
       "8                      XGBoost Classifier   \n",
       "9                     Logistic Regression   \n",
       "10                    LightGBM Classifier   \n",
       "11                RandomForest Classifier   \n",
       "12                             LSTM+Dense   \n",
       "13                   bert_pretained_model   \n",
       "14  TFDistilBertForSequenceClassification   \n",
       "15  TFDistilBertForSequenceClassification   \n",
       "\n",
       "                           Text Features Type Train Acccurcay CV_accuracy  \\\n",
       "1        Bag of word(Unigram)+sentiment_score               -           -   \n",
       "2        Bag of word(Unigram)+sentiment_score               -           -   \n",
       "3        Bag of word(Unigram)+sentiment_score               -           -   \n",
       "4        Bag of word(Unigram)+sentiment_score           0.659           -   \n",
       "5         Bag of word(Bigram)+sentiment_score               -           -   \n",
       "6         Bag of word(Bigram)+sentiment_score               -           -   \n",
       "7         Bag of word(Bigram)+sentiment_score               -           -   \n",
       "8         Bag of word(Bigram)+sentiment_score           0.602           -   \n",
       "9          TF-IDF(Uni+Bigram)+sentiment_score               -           -   \n",
       "10         TF-IDF(Uni+Bigram)+sentiment_score               -           -   \n",
       "11         TF-IDF(Uni+Bigram)+sentiment_score               -           -   \n",
       "12                       Text_sequnece_padded           0.705           -   \n",
       "13                            Bert Tokeinizer           0.652           -   \n",
       "14      DistilBertTokenizer(for cleaned text)           0.773       0.821   \n",
       "15  DistilBertTokenizer(without cleaned text)           0.785       0.827   \n",
       "\n",
       "    Test_accuracy  \n",
       "1           0.691  \n",
       "2           0.720  \n",
       "3           0.690  \n",
       "4           0.657  \n",
       "5           0.695  \n",
       "6           0.694  \n",
       "7           0.619  \n",
       "8           0.599  \n",
       "9           0.695  \n",
       "10          0.694  \n",
       "11          0.626  \n",
       "12          0.710  \n",
       "13          0.660  \n",
       "14          0.757  \n",
       "15          0.771  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_compare = pd.DataFrame([['Logistic Regression',\"Bag of word(Unigram)+sentiment_score\",\"-\",\"-\" ,0.691,],\n",
    "                               ['LightGBM Classifier',\"Bag of word(Unigram)+sentiment_score\",\"-\",\"-\" ,0.720],\n",
    "                               ['RandomForest Classifier',\"Bag of word(Unigram)+sentiment_score\",\"-\",\"-\" ,0.69],\n",
    "                               ['XGBoost Classifier',\"Bag of word(Unigram)+sentiment_score\",\"0.659\",\"-\" ,0.657],\n",
    "                               ['Logistic Regression',\"Bag of word(Bigram)+sentiment_score\",\"-\",\"-\" ,0.695,],\n",
    "                               ['LightGBM Classifier',\"Bag of word(Bigram)+sentiment_score\",\"-\",\"-\" ,0.694],\n",
    "                               ['RandomForest Classifier',\"Bag of word(Bigram)+sentiment_score\",\"-\",\"-\" ,0.619],\n",
    "                               ['XGBoost Classifier',\"Bag of word(Bigram)+sentiment_score\",\"0.602\",\"-\" ,0.599],\n",
    "                               ['Logistic Regression',\"TF-IDF(Uni+Bigram)+sentiment_score\",\"-\",\"-\" ,0.695,],\n",
    "                               ['LightGBM Classifier',\"TF-IDF(Uni+Bigram)+sentiment_score\",\"-\",\"-\" ,0.694],\n",
    "                               ['RandomForest Classifier',\"TF-IDF(Uni+Bigram)+sentiment_score\",\"-\",\"-\" ,0.626],\n",
    "                               ['LSTM+Dense',\"Text_sequnece_padded\",0.705,\"-\" ,0.710],\n",
    "                               ['bert_pretained_model',\"Bert Tokeinizer\",0.652,\"-\" ,0.660],\n",
    "                               ['TFDistilBertForSequenceClassification',\"DistilBertTokenizer(for cleaned text)\",0.773,0.821 ,0.757],\n",
    "                               ['TFDistilBertForSequenceClassification',\"DistilBertTokenizer(without cleaned text)\",0.785,0.827 ,0.771]],\n",
    "                              columns = ['Model_Name','Text Features Type',\"Train Acccurcay\",\"CV_accuracy\",\"Test_accuracy\" ], index = list(range(1,16)))\n",
    "\n",
    "display(result_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "61a50a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ./model/clf_distilBert_no_clean were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at ./model/clf_distilBert_no_clean and are newly initialized: ['dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained('./model/clf_distilBert_no_clean')\n",
    "model_name, max_len = pickle.load(open('./model/info.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa3a119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tkzr = DistilBertTokenizer.from_pretrained(model_name)\n",
    "# joblib.dump(tkzr,\"./model/clf_distilBert_no_clean/tkzr.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d6d91bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tflite_model_name   = \"tflite_distilbert_mdoel.tflite\"\n",
    "# open(tflite_model_name, \"wb\").write(tflite_distilbert_mdoel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4acd0d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.lite.Interpreter(model_path = tflite_model_name)\n",
    "# input_details = model.get_input_details()\n",
    "# output_details = model.get_output_details()\n",
    "# print(\"Input Shape:\", input_details[0]['shape'])\n",
    "# print(\"Input Type:\", input_details[0]['dtype'])\n",
    "# print(\"Output Shape:\", output_details[0]['shape'])\n",
    "# print(\"Output Type:\", output_details[0]['dtype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "33b8ae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #interpreter.resize_tensor_input(input_details[0]['index'], (10000, 28, 28))\n",
    "# #interpreter.resize_tensor_input(output_details[0]['index'], (10000, 10))\n",
    "# model.allocate_tensors()\n",
    "# input_details = model.get_input_details()\n",
    "# output_details = model.get_output_details()\n",
    "# print(\"Input Shape:\", input_details[0]['shape'])\n",
    "# print(\"Input Type:\", input_details[0]['dtype'])\n",
    "# print(\"Output Shape:\", output_details[0]['shape'])\n",
    "# print(\"Output Type:\", output_details[0]['dtype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "486fd3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "tkzr= joblib.load(\"./model/clf_distilBert_no_clean/tkzr.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f16fe29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_encodings(x, tkzr, max_len, trucation=True, padding=True):\n",
    "    return tkzr(x, max_length=max_len, truncation=trucation, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d8ff401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_tfdataset(encodings, y=None):\n",
    "    if y:\n",
    "        return tf.data.Dataset.from_tensor_slices((dict(encodings),y))\n",
    "    else:\n",
    "        # this case is used when making predictions on unseen samples after training\n",
    "        return tf.data.Dataset.from_tensor_slices(dict(encodings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9368a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictor(model,tkzr, max_len):\n",
    "    \n",
    "    #tkzr = DistilBertTokenizer.from_pretrained(model_name)\n",
    "    tkzr= tkzr\n",
    "    \n",
    "    def predict_proba(text):\n",
    "        \n",
    "        x = [text]\n",
    "\n",
    "        encodings = construct_encodings(x, tkzr, max_len=max_len)\n",
    "        tfdataset = construct_tfdataset(encodings)\n",
    "        tfdataset = tfdataset.batch(1)\n",
    "\n",
    "        preds = model.predict(tfdataset).logits\n",
    "        preds = activations.softmax(tf.convert_to_tensor(preds)).numpy()\n",
    "        return preds[0]\n",
    "\n",
    "    return predict_proba\n",
    "\n",
    "\n",
    "\n",
    "#prediction=clf(\"You mean he doesnt get to retroactively remove consent like women with buyers remorse are allowed to?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "29714772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a text to check if it is Sarcastic : \n",
      " \n",
      " You mean he doesnt get to retroactively remove consent like women with buyers remorse are allowed to?\n",
      "\n",
      "\n",
      "Probablity of this text to be non sarcastic: 0.12029655277729034\n",
      "\n",
      "\n",
      "Probablity of this text to be sarcastic    : 0.8797035217285156\n",
      "\n",
      "\n",
      "We have detected Sarcasm in this comment\n"
     ]
    }
   ],
   "source": [
    "# try:\n",
    "Sample_text= str(input(\"Enter a text to check if it is Sarcastic : \\n \\n \"))\n",
    "print(\"\\n\") \n",
    "clf = create_predictor(model, tkzr, max_len)\n",
    "prediction=clf(Sample_text)\n",
    "print(f\"Probablity of this text to be non sarcastic: {prediction[0]}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Probablity of this text to be sarcastic    : {prediction[1]}\")\n",
    "print(\"\\n\")\n",
    "label_= np.argmax(np.array(prediction))\n",
    "#out=\"\"\n",
    "if label_==1:\n",
    "    print(\"We have detected Sarcasm in this comment\")\n",
    "else:\n",
    "    print(\"We couldn't detected 'Sarcasm' in this comment\")\n",
    "    \n",
    "# except:\n",
    "#     print(\"Please enter a string \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e32e7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
